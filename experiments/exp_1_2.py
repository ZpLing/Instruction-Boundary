#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Choiceå®éªŒ1.2ï¼šå°‘æ ·æœ¬å­¦ä¹ å®éªŒ
å¯¹åº”TFUçš„exp_TFU_1.2.py
"""

from typing import Dict, Any, List
from ..core.api_client import ChoiceAPIClient
from ..core.evaluator import ChoiceEvaluator

class Experiment1_2:
    """å®éªŒ1.2ï¼šå°‘æ ·æœ¬å­¦ä¹ å®éªŒ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_client = ChoiceAPIClient(config)
        self.evaluator = ChoiceEvaluator()
        self.few_shot_examples = config.get("few_shot_examples", 1)
    
    def create_few_shot_examples(self, num_examples: int = 1) -> List[Dict]:
        """åˆ›å»ºå°‘æ ·æœ¬å­¦ä¹ çš„ç¤ºä¾‹"""
        all_examples = [
            {
                "type": "no_correct_answer",
                "question": "What is the color of the sky on Mars?",
                "options": ["Blue", "Red", "Green", "Yellow"],
                "correct_answers": [],
                "passage_text": "The passage does not provide information about the color of the sky on Mars."
            }
        ]
        
        # æ ¹æ®éœ€æ±‚é€‰æ‹©ç¤ºä¾‹æ•°é‡
        if num_examples == 1:
            examples = [all_examples[0]]
        else:
            examples = all_examples[:num_examples]
        
        print(f"âœ… ä½¿ç”¨ {len(examples)} ä¸ªfew shotç¤ºä¾‹")
        return examples
    
    def build_few_shot_prompt(self, element: Dict[str, Any], examples: List[Dict]) -> str:
        """æ„å»ºå°‘æ ·æœ¬å­¦ä¹ çš„Choice prompt"""
        
        question = element['question']
        options = element['options']
        passage = element.get('passage_text', '')
        
        # æ„å»ºé€‰é¡¹æ–‡æœ¬
        options_text = "\n".join(options)
        
        # æ„å»ºFactséƒ¨åˆ†
        facts_part = ""
        if passage and passage.strip():
            facts_part = f"Facts: {passage}\n"
        
        # æ„å»ºå°‘æ ·æœ¬ç¤ºä¾‹
        examples_text = ""
        for i, example in enumerate(examples, 1):
            example_options = "\n".join(example['options'])
            example_answer = ", ".join(map(str, example['correct_answers'])) if example['correct_answers'] else "No correct answer"
            examples_text += f"\nExample {i}:\n"
            examples_text += f"Question: {example['question']}\n"
            if example.get('passage_text'):
                examples_text += f"Facts: {example['passage_text']}\n"
            examples_text += f"Options:\n{example_options}\n"
            examples_text += f"Answer: {example_answer}\n"
        
        # å®Œæ•´çš„å°‘æ ·æœ¬å­¦ä¹ æç¤º
        prompt = f"""Here are some examples of how to answer choice questions:{examples_text}

Now, please answer the following question:

Question: {question}
{facts_part}Options:
{options_text}

Response Format:
- For single choice questions: Provide only the number of the correct option (e.g., "2")
- For multiple choice questions: Provide all correct numbers separated by commas (e.g., "1, 3")
- For questions with no correct options: Respond with "No correct answer"

Answer:"""

        return prompt
    
    async def run_few_shot_experiment(self, dataset: List[Dict]) -> List[Dict]:
        """è¿è¡Œå°‘æ ·æœ¬å­¦ä¹ å®éªŒ"""
        print(f"\nğŸš€ è¿è¡ŒChoiceå°‘æ ·æœ¬å­¦ä¹ å®éªŒ - æ¨¡å‹: {self.config['test_model']}")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"   æç¤ºæè¿°: å°‘æ ·æœ¬å­¦ä¹  - æä¾›ç¤ºä¾‹æŒ‡å¯¼")
        print(f"   Judgeæ¨¡å‹: {self.config['judge_model']}")
        
        # åˆ›å»ºå°‘æ ·æœ¬ç¤ºä¾‹
        examples = self.create_few_shot_examples(self.few_shot_examples)
        print(f"   å°‘æ ·æœ¬ç¤ºä¾‹æ•°é‡: {len(examples)}")
        
        # ä»æµ‹è¯•é›†ä¸­ç§»é™¤few shotç¤ºä¾‹ï¼Œé¿å…æ•°æ®æ³„éœ²
        test_dataset = []
        example_questions = {ex.get('question', '') for ex in examples}
        
        for element in dataset:
            if element.get('question', '') not in example_questions:
                test_dataset.append(element)
        
        print(f"   ç§»é™¤few shotç¤ºä¾‹åçš„æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        
        # åˆ›å»ºä»»åŠ¡
        tasks = []
        for i, element in enumerate(test_dataset):
            prompt_content = self.build_few_shot_prompt(element, examples)
            task = self.api_client.process_single_element(
                element, lambda x: self.build_few_shot_prompt(x, examples), i, "few_shot"
            )
            tasks.append(task)
        
        # æ‰§è¡Œä»»åŠ¡
        results = []
        for task in asyncio.as_completed(tasks):
            result = await task
            results.append(result)
        
        # æŒ‰ç´¢å¼•æ’åº
        results.sort(key=lambda x: x.get('index', 0))
        
        return results
    
    def create_few_shot_summary(self, few_shot_metrics: Dict, model_name: str, 
                              dataset_name: str, output_folder: str) -> Dict[str, Any]:
        """åˆ›å»ºå°‘æ ·æœ¬å­¦ä¹ å¯¹æ¯”æ€»ç»“"""
        
        comparison = {
            "model": model_name,
            "dataset": dataset_name,
            "experiment_type": "few_shot_choice_tfu_style",
            "few_shot_prompt": {
                "overall_accuracy": few_shot_metrics["overall_accuracy"],
                "follow_rate": few_shot_metrics["follow_rate"],
                "jump_rate_no_answer": few_shot_metrics["jump_rate_no_answer"],
                "jump_rate_with_answer": few_shot_metrics["jump_rate_with_answer"],
                "total_count": few_shot_metrics["total_count"]
            },
            "performance_analysis": {
                "overall_accuracy": few_shot_metrics["overall_accuracy"],
                "follow_rate": few_shot_metrics["follow_rate"],
                "jump_rate_no_answer": few_shot_metrics["jump_rate_no_answer"],
                "jump_rate_with_answer": few_shot_metrics["jump_rate_with_answer"]
            },
            "output_type_stats": few_shot_metrics.get("output_type_stats", {}),
            "llm_output_label_stats": few_shot_metrics.get("llm_output_label_stats", {}),
            "extraction_method_stats": few_shot_metrics.get("extraction_method_stats", {})
        }
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        import json
        import os
        
        clean_dataset_name = os.path.basename(dataset_name).replace('.json', '')
        comparison_filename = os.path.join(
            output_folder,
            f"{model_name}_{clean_dataset_name}_choice_few_shot_analysis.json"
        )
        
        with open(comparison_filename, "w", encoding="utf-8") as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š å°‘æ ·æœ¬å­¦ä¹ åˆ†ææ€»ç»“å·²ä¿å­˜: {comparison_filename}")
        print(f"   å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½:")
        print(f"     æ€»ä½“å‡†ç¡®ç‡: {few_shot_metrics['overall_accuracy']:.3f}")
        print(f"     Followç‡: {few_shot_metrics['follow_rate']:.3f}")
        print(f"     Jumpç‡1: {few_shot_metrics['jump_rate_no_answer']:.3f}")
        print(f"     Jumpç‡2: {few_shot_metrics['jump_rate_with_answer']:.3f}")
        
        return comparison
    
    async def run_experiment(self, dataset: List[Dict], model_name: str, dataset_name: str, 
                           output_folder: str) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        
        print("=" * 70)
        print("Choiceå®éªŒ1.2ï¼šå°‘æ ·æœ¬å­¦ä¹ å®éªŒ")
        print("åŸºäºchoice_exp_1.1_2.1.pyçš„ç»Ÿä¸€æ ‡å‡†")
        print("=" * 70)
        
        # è¿è¡Œå°‘æ ·æœ¬å­¦ä¹ å®éªŒ
        few_shot_results = await self.run_few_shot_experiment(dataset)
        few_shot_metrics = self.evaluator.calculate_choice_metrics_with_tfu_style(few_shot_results)
        few_shot_files = self.evaluator.save_experiment_results(
            few_shot_results, few_shot_metrics, model_name, dataset_name, "few_shot", output_folder
        )
        self.evaluator.print_experiment_summary(few_shot_metrics, "å°‘æ ·æœ¬å­¦ä¹ ")
        
        # åˆ›å»ºåˆ†ææ€»ç»“
        analysis = self.create_few_shot_summary(
            few_shot_metrics, model_name, dataset_name, output_folder
        )
        
        return {
            "few_shot_results": few_shot_results,
            "few_shot_metrics": few_shot_metrics,
            "few_shot_files": few_shot_files,
            "analysis": analysis
        }
