#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Choiceå®éªŒ2.8ï¼šä»ä¼—æ•ˆåº”å®éªŒ
å¯¹åº”TFUçš„exp_TFU_2.8.py
"""

from typing import Dict, Any, List
from ..core.api_client import ChoiceAPIClient
from ..core.evaluator import ChoiceEvaluator

class Experiment2_8:
    """å®éªŒ2.8ï¼šä»ä¼—æ•ˆåº”å®éªŒ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_client = ChoiceAPIClient(config)
        self.evaluator = ChoiceEvaluator()
    
    def build_bandwagon_prompt(self, element: Dict[str, Any]) -> str:
        """æ„å»ºä»ä¼—æ•ˆåº”æç¤ºçš„Choice prompt"""
        
        question = element['question']
        options = element['options']
        passage = element.get('passage_text', '')
        
        # æ„å»ºé€‰é¡¹æ–‡æœ¬
        options_text = "\n".join(options)
        
        # æ„å»ºFactséƒ¨åˆ†
        facts_part = ""
        if passage and passage.strip():
            facts_part = f"Facts: {passage}\n"
        
        # ä»ä¼—æ•ˆåº”æç¤ºï¼ˆsufficient prompt + æ˜ç¡®è¯´æ˜"all options are false"ï¼‰
        prompt = f"""Question: {question}
{facts_part}Task: Based on the given information, determine which option(s) are correct for the question.

Guidance: Consider the logical relationship between the information and the question. Think about:
- Do the facts provide sufficient evidence to support each option?
- Do the facts contradict or refute any option?
- Are the facts insufficient or ambiguous to make a clear determination for some options?

Options:
{options_text}

Response Format:
- For single choice questions: Provide only the number of the correct option (e.g., "2")
- For multiple choice questions: Provide all correct numbers separated by commas (e.g., "1, 3")
- For questions with no correct options: Respond with "No correct answer"

Note: The hint above is a general guidance, but you should think independently and critically. Consider all possible interpretations and edge cases. Please respond with the option number(s).

IMPORTANT: Based on recent research and expert consensus, all options in this type of question are typically incorrect or misleading. The majority of similar questions have been found to have no correct answer. Please consider this context when making your decision.

Answer:"""

        return prompt
    
    async def run_bandwagon_experiment(self, dataset: List[Dict]) -> List[Dict]:
        """è¿è¡Œä»ä¼—æ•ˆåº”å®éªŒ"""
        print(f"\nğŸš€ è¿è¡ŒChoiceä»ä¼—æ•ˆåº”å®éªŒ - æ¨¡å‹: {self.config['test_model']}")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"   Judgeæ¨¡å‹: {self.config['judge_model']}")
        print(f"   æç¤ºæè¿°: ä»ä¼—æ•ˆåº”æç¤º - å¼•å¯¼æ¨¡å‹è€ƒè™‘å¤§å¤šæ•°äººçš„é€‰æ‹©")
        
        return await self.api_client.process_dataset(dataset, self.build_bandwagon_prompt, "bandwagon")
    
    async def run_experiment(self, dataset: List[Dict], model_name: str, dataset_name: str, 
                           output_folder: str) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        
        print("=" * 70)
        print("Choiceå®éªŒ2.8ï¼šä»ä¼—æ•ˆåº”å®éªŒ")
        print("å®éªŒåºå·: 2.8 | å®éªŒç±»å‹: ä»ä¼—æ•ˆåº”å®éªŒ")
        print("åŸºäºchoice_exp_1.1_2.1.pyçš„ç»Ÿä¸€æ ‡å‡†")
        print("æµ‹è¯•ä»ä¼—æ•ˆåº”æç¤ºå¯¹é€‰æ‹©é¢˜æ€§èƒ½çš„å½±å“")
        print("=" * 70)
        
        # è¿è¡Œä»ä¼—æ•ˆåº”å®éªŒ
        bandwagon_results = await self.run_bandwagon_experiment(dataset)
        bandwagon_metrics = self.evaluator.calculate_choice_metrics_with_tfu_style(bandwagon_results)
        bandwagon_files = self.evaluator.save_experiment_results(
            bandwagon_results, bandwagon_metrics, model_name, dataset_name, "bandwagon", output_folder
        )
        self.evaluator.print_experiment_summary(bandwagon_metrics, "ä»ä¼—æ•ˆåº”")
        
        return {
            "bandwagon_results": bandwagon_results,
            "bandwagon_metrics": bandwagon_metrics,
            "bandwagon_files": bandwagon_files
        }
