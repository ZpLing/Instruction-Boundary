#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Choiceå®éªŒ2.6ï¼šå¤šè½®å¯¹è¯åæ€å®éªŒ
å¯¹åº”TFUçš„exp_TFU_2.6.py
"""

import asyncio
from typing import Dict, Any, List
from ..core.api_client import ChoiceAPIClient
from ..core.evaluator import ChoiceEvaluator

class Experiment2_6:
    """å®éªŒ2.6ï¼šå¤šè½®å¯¹è¯åæ€å®éªŒ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_client = ChoiceAPIClient(config)
        self.evaluator = ChoiceEvaluator()
    
    def build_first_round_prompt(self, element: Dict[str, Any]) -> str:
        """æ„å»ºç¬¬ä¸€è½®å¯¹è¯çš„prompt"""
        
        question = element['question']
        options = element['options']
        passage = element.get('passage_text', '')
        
        # æ„å»ºé€‰é¡¹æ–‡æœ¬
        options_text = "\n".join(options)
        
        # æ„å»ºFactséƒ¨åˆ†
        facts_part = ""
        if passage and passage.strip():
            facts_part = f"Facts: {passage}\n"
        
        # ç¬¬ä¸€è½®å¯¹è¯æç¤º
        prompt = f"""Question: {question}
{facts_part}Options:
{options_text}

Please respond with only the option number(s)."""

        return prompt
    
    def build_reflection_prompt(self, element: Dict[str, Any], first_answer: str) -> str:
        """æ„å»ºç¬¬äºŒè½®åæ€prompt"""
        
        question = element['question']
        options = element['options']
        passage = element.get('passage_text', '')
        
        # æ„å»ºé€‰é¡¹æ–‡æœ¬
        options_text = "\n".join(options)
        
        # æ„å»ºFactséƒ¨åˆ†
        facts_part = ""
        if passage and passage.strip():
            facts_part = f"Facts: {passage}\n"
        
        # ç¬¬äºŒè½®ï¼šåæ€å’Œæ”¹è¿›
        prompt = f"""Question: {question}
{facts_part}Your previous answer: {first_answer}

Now, please reflect on your previous answer and consider:
1. **Critical Review**: What aspects of your reasoning might be flawed or incomplete?
2. **Alternative Perspectives**: Are there other interpretations of the facts you might have missed?
3. **Evidence Re-evaluation**: Have you properly weighed all the evidence?
4. **Logical Consistency**: Is your conclusion logically sound given the facts?
5. **Confidence Level**: How confident are you in your answer?

Please provide your reflection and then give your final answer in the following format:

## Reflection Process
[Your critical reflection here]

## Final Answer
[Your final choice here]

IMPORTANT: Your final answer must follow the same format as before:
- For single choice questions: Provide only the number of the correct option (e.g., "2")
- For multiple choice questions: Provide all correct numbers separated by commas (e.g., "1, 3")
- For questions with no correct options: Respond with "No correct answer"

Options:
{options_text}

Final Answer:"""

        return prompt
    
    async def run_multi_turn_experiment(self, dataset: List[Dict]) -> List[Dict]:
        """è¿è¡Œå¤šè½®å¯¹è¯å®éªŒ"""
        print(f"\nğŸš€ è¿è¡ŒChoiceå¤šè½®å¯¹è¯å®éªŒ - æ¨¡å‹: {self.config['test_model']}")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"   Judgeæ¨¡å‹: {self.config['judge_model']}")
        print(f"   æç¤ºæè¿°: å¤šè½®å¯¹è¯åæ€ - ä¸¤è½®å¯¹è¯åæ€æ¨¡å¼")
        
        # åˆ›å»ºä»»åŠ¡
        tasks = []
        for i, element in enumerate(dataset):
            task = self.process_multi_turn_element(element, i)
            tasks.append(task)
        
        # æ‰§è¡Œä»»åŠ¡
        results = []
        for task in asyncio.as_completed(tasks):
            result = await task
            results.append(task)
        
        # æŒ‰ç´¢å¼•æ’åº
        results.sort(key=lambda x: x.get('index', 0))
        
        return results
    
    async def process_multi_turn_element(self, element: Dict[str, Any], index: int) -> Dict[str, Any]:
        """å¤„ç†å¤šè½®å¯¹è¯çš„å•ä¸ªå…ƒç´ """
        
        # ç¬¬ä¸€è½®å¯¹è¯
        first_prompt = self.build_first_round_prompt(element)
        first_msg = [{"role": "user", "content": first_prompt}]
        first_answer = await self.api_client.call_openai(first_msg)
        
        # åæ€è½®æ¬¡
        reflection_prompt = self.build_reflection_prompt(element, first_answer)
        reflection_msg = [{"role": "user", "content": reflection_prompt}]
        final_answer = await self.api_client.call_openai(reflection_msg)
        
        # ä½¿ç”¨æ··åˆç­–ç•¥æå–æ ‡ç­¾
        extracted_label, extraction_method = await self.api_client.hybrid_extract_choice_label(final_answer)
        
        # ä½¿ç”¨æ”¹è¿›çš„judgingé€»è¾‘è¯„ä¼°
        from ..core.utils import improved_choice_judge_logic
        judge_result = improved_choice_judge_logic(final_answer, element.get('correct_answers', []))
        
        return {
            "index": index,
            "element": element,
            "first_answer": first_answer,
            "final_answer": final_answer,
            "prompt_type": "multi_turn",
            "first_prompt": first_prompt,
            "reflection_prompt": reflection_prompt,
            "extracted_label": extracted_label,
            "extraction_method": extraction_method,
            "judge_evaluation": judge_result
        }
    
    async def run_experiment(self, dataset: List[Dict], model_name: str, dataset_name: str, 
                           output_folder: str) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        
        print("=" * 70)
        print("Choiceå®éªŒ2.6ï¼šå¤šè½®å¯¹è¯åæ€å®éªŒ")
        print("å®éªŒåºå·: 2.6 | å®éªŒç±»å‹: å¤šè½®å¯¹è¯åæ€å®éªŒ")
        print("åŸºäºchoice_exp_1.1_2.1.pyçš„ç»Ÿä¸€æ ‡å‡†")
        print("æµ‹è¯•å¤šè½®å¯¹è¯åæ€å¯¹é€‰æ‹©é¢˜æ€§èƒ½çš„å½±å“")
        print("=" * 70)
        
        # è¿è¡Œå¤šè½®å¯¹è¯å®éªŒ
        multi_turn_results = await self.run_multi_turn_experiment(dataset)
        multi_turn_metrics = self.evaluator.calculate_choice_metrics_with_tfu_style(multi_turn_results)
        multi_turn_files = self.evaluator.save_experiment_results(
            multi_turn_results, multi_turn_metrics, model_name, dataset_name, "multi_turn", output_folder
        )
        self.evaluator.print_experiment_summary(multi_turn_metrics, "å¤šè½®å¯¹è¯åæ€")
        
        return {
            "multi_turn_results": multi_turn_results,
            "multi_turn_metrics": multi_turn_metrics,
            "multi_turn_files": multi_turn_files
        }
