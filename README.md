# ğŸ¯ Choice Experiment Toolkit

<div align="center">

**A comprehensive toolkit for evaluating LLM instruction following capabilities through multiple choice experiments**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o-green.svg)](https://openai.com/)

</div>

This toolkit implements **6 experimental settings** that systematically evaluate how Large Language Models (LLMs) handle instruction following in multiple choice scenarios. The experiments are adapted from the original TFU (Truth Following Under Uncertainty) framework, providing a standardized approach to assess model behavior across different prompt strategies and cognitive biases.

## âœ¨ Key Features

- ğŸ”¬ **6 Comprehensive Experiments**: Systematic evaluation of instruction following capabilities
- ğŸ¨ **Dual Format Support**: Automatic detection and conversion between Choice and TFU formats
- ğŸ¤– **Multi-Model Support**: Compatible with GPT-4o, Claude, Llama, and Gemini
- ğŸ“Š **Rich Evaluation Metrics**: Follow rate, Jump rate, and TFU-style analysis
- ğŸš€ **Easy-to-Use**: Command-line interface and Python API
- ğŸ“ˆ **Detailed Outputs**: Comprehensive results with statistical analysis

## ğŸš€ Quick Start

### ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/ZpLing/Instruction-Boundary.git
cd Instruction-Boundary/choice_toolkit

# Install dependencies
pip install -r requirements.txt
```

### ğŸ”‘ API Configuration

```bash
# Set your OpenAI API key
export OPENAI_API_KEY="your-api-key-here"

# Optional: Set custom base URL (for custom API endpoints)
export OPENAI_BASE_URL="https://api.openai.com/v1"
```

> âš ï¸ **Security Note**: Never commit your actual API key to the repository. Always use environment variables.

### ğŸ® Running Experiments

```bash
# ğŸ“‹ List all available experiments
python main.py --list

# ğŸš€ Run all experiments
python main.py --all

# ğŸ¯ Run specific experiments
python main.py --experiment 1.1_2.1,2.3

# ğŸ¤– Run with specific model
python main.py --model gpt-4o --experiment 1.1_2.1

# ğŸ“Š Quick test (recommended for first run)
python test_imports.py
```

### âš¡ 30-Second Demo

```bash
# Quick start with interactive menu
python quick_start.py
```


## ğŸ§ª Experimental Settings

The toolkit implements **6 comprehensive experimental settings** that systematically evaluate different aspects of LLM instruction following behavior:

<div align="center">

| ğŸ¯ Experiment | ğŸ“ Description | ğŸ”¬ Focus Area | ğŸ“Š Original TFU |
|---------------|----------------|---------------|-----------------|
| **1.1_2.1** | Sufficient vs Insufficient Prompts | Information Completeness | exp_TFU_1.1_2.1.py |
| **1.2** | Few-shot Learning | Learning from Examples | exp_TFU_1.2.py |
| **2.3** | Ambiguous Prompts | Uncertainty Handling | exp_TFU_2.3.py |
| **2.5** | LLM Polished Prompts | Prompt Optimization | exp_TFU_2.5.py |
| **2.6** | Multi-turn Conversation | Self-Reflection | exp_TFU_2.6.py |
| **2.8** | Bandwagon Effect | Social Bias | exp_TFU_2.8.py |

</div>

### ğŸ”¬ Detailed Experimental Descriptions

#### ğŸ¯ **1.1_2.1 - Sufficient vs Insufficient Prompts**
- **Objective**: Evaluate how information completeness affects model performance
- **Method**: Compare responses with full context vs. missing critical information
- **Metrics**: Accuracy difference, confidence levels, error patterns
- **Insight**: Tests model's dependency on complete information

#### ğŸ“ **1.2 - Few-shot Learning**
- **Objective**: Assess model's ability to learn from examples
- **Method**: Provide 1-3 example demonstrations before target questions
- **Metrics**: Learning curve, example utilization, generalization
- **Insight**: Evaluates in-context learning capabilities

#### â“ **2.3 - Ambiguous Prompts**
- **Objective**: Test model behavior with unclear instructions
- **Method**: Use minimal, vague prompts with reduced guidance
- **Metrics**: Uncertainty handling, interpretation consistency
- **Insight**: Measures robustness to ambiguous inputs

#### âœ¨ **2.5 - LLM Polished Prompts**
- **Objective**: Test if model-generated prompts improve performance
- **Method**: Use LLM-refined versions of insufficient prompts
- **Metrics**: Prompt effectiveness, self-improvement capability
- **Insight**: Evaluates model's prompt optimization skills

#### ğŸ”„ **2.6 - Multi-turn Conversation**
- **Objective**: Assess self-reflection and iterative improvement
- **Method**: Two-round conversations with reflection prompts
- **Metrics**: Improvement rate, consistency, self-correction
- **Insight**: Tests reasoning and self-correction abilities

#### ğŸ‘¥ **2.8 - Bandwagon Effect**
- **Objective**: Evaluate susceptibility to social bias
- **Method**: Include misleading hints about "popular" answers
- **Metrics**: Conformity rate, bias resistance, critical thinking
- **Insight**: Tests independence from social influence

## ğŸ“ Project Structure

```
choice_toolkit/
â”œâ”€â”€ ğŸ“ config/                          # Configuration files
â”‚   â”œâ”€â”€ model_config.py                 # Model and API settings
â”‚   â””â”€â”€ experiment_config.py            # Experiment parameters
â”œâ”€â”€ ğŸ“ core/                            # Core functionality
â”‚   â”œâ”€â”€ api_client.py                   # API communication
â”‚   â”œâ”€â”€ data_loader.py                  # Dataset loading and processing
â”‚   â”œâ”€â”€ evaluator.py                    # Evaluation metrics
â”‚   â””â”€â”€ utils.py                        # Utility functions
â”œâ”€â”€ ğŸ“ experiments/                     # Individual experiment implementations
â”‚   â”œâ”€â”€ exp_1_1_2_1.py                 # Sufficient vs Insufficient prompts
â”‚   â”œâ”€â”€ exp_1_2.py                      # Few-shot learning
â”‚   â”œâ”€â”€ exp_2_3.py                      # Ambiguous prompts
â”‚   â”œâ”€â”€ exp_2_5.py                      # LLM polished prompts
â”‚   â”œâ”€â”€ exp_2_6.py                      # Multi-turn conversation
â”‚   â””â”€â”€ exp_2_8.py                      # Bandwagon effect
â”œâ”€â”€ ğŸ“ results/                         # Output directories (auto-created)
â”œâ”€â”€ ğŸ“„ main.py                          # Main execution script
â”œâ”€â”€ ğŸ“„ quick_start.py                   # Interactive quick start
â”œâ”€â”€ ğŸ“„ example_usage.py                 # Usage examples
â”œâ”€â”€ ğŸ“„ test_imports.py                  # Import validation
â”œâ”€â”€ ğŸ“„ test_toolkit.py                  # Comprehensive tests
â”œâ”€â”€ ğŸ“Š mixed_450_qa_dataset.json        # Choice format dataset (450 samples)
â”œâ”€â”€ ğŸ“Š choice_tfu_format_dataset.json   # TFU format dataset (450 samples)
â”œâ”€â”€ âš™ï¸ choice_config.json               # Dataset configuration
â”œâ”€â”€ ğŸ“‹ requirements.txt                 # Python dependencies
â””â”€â”€ ğŸ“– README.md                        # This file
```

## ğŸ“Š Dataset Formats

The toolkit supports **dual dataset formats** with automatic detection and seamless conversion:

### ğŸ¯ **Choice Format** (Standard Multiple Choice)
```json
{
  "question": "What is the capital of France?",
  "options": ["London", "Berlin", "Paris", "Madrid"],
  "correct_answers": [2],
  "question_type": "single_choice"
}
```
- **File**: `mixed_450_qa_dataset.json`
- **Style**: Direct question-answer with option selection
- **Use Case**: Standard multiple choice evaluation

### ğŸ” **TFU Format** (Evidence-Based Reasoning)
```json
{
  "Conclusion": "Paris is the capital of France",
  "Facts": "Paris is located in northern France and serves as the political center...",
  "correct_answers": [0],
  "question_type": "single_choice"
}
```
- **File**: `choice_tfu_format_dataset.json`
- **Style**: Evidence-based reasoning (proven/disproven/undetermined)
- **Use Case**: TFU-style cognitive evaluation

### ğŸ”„ **Automatic Format Detection**
The toolkit intelligently detects format based on field presence:
- **Choice format**: `question` + `options` fields
- **TFU format**: `Conclusion` + `Facts` fields
- **Auto-conversion**: TFU format internally converted to Choice format

## ğŸ“ˆ Output & Results

Results are automatically saved in the `results/` directory with comprehensive analysis:

```
results/
â”œâ”€â”€ ğŸ“ experiment_data_choice_1_2/           # Experiment 1.1_2.1 results
â”‚   â”œâ”€â”€ ğŸ“„ gpt-4o_mixed_450_qa_sufficient_evaluation.json
â”‚   â”œâ”€â”€ ğŸ“„ gpt-4o_mixed_450_qa_insufficient_evaluation.json
â”‚   â””â”€â”€ ğŸ“Š gpt-4o_mixed_450_qa_sufficient_vs_insufficient_comparison.json
â”œâ”€â”€ ğŸ“ experiment_data_choice_2_3/           # Experiment 2.3 results
â”‚   â””â”€â”€ ğŸ“„ gpt-4o_mixed_450_qa_ambiguous_evaluation.json
â””â”€â”€ ğŸ“ experiment_data_choice_2_6/           # Experiment 2.6 results
    â””â”€â”€ ğŸ“„ gpt-4o_mixed_450_qa_multi_turn_evaluation.json
```

### ğŸ“Š **Output File Types**

| ğŸ“„ File Type | ğŸ“ Description | ğŸ” Content |
|--------------|----------------|------------|
| **Evaluation Results** | Detailed responses and analysis | Model outputs, extracted labels, judge evaluations |
| **Accuracy Analysis** | Performance metrics | Follow rate, Jump rate, overall accuracy |
| **Comparison Summaries** | Cross-experiment analysis | Statistical comparisons, performance differences |
| **TFU-style Metrics** | Specialized evaluation | Cognitive bias analysis, reasoning patterns |

### ğŸ“ˆ **Key Metrics**

- **ğŸ¯ Follow Rate**: Percentage of correct single-choice answers
- **ğŸš€ Jump Rate (No Answer)**: Accuracy on "no correct answer" questions  
- **ğŸš€ Jump Rate (Multiple)**: Accuracy on multiple-choice questions
- **ğŸ“Š Overall Accuracy**: Total correctness across all question types
- **ğŸ” Extraction Method Stats**: Keyword vs LLM judge usage
- **ğŸ“‹ Output Distribution**: Response pattern analysis

## ğŸ’» Usage Examples

### ğŸš€ **Basic Python Usage**

```python
import asyncio
from main import ChoiceToolkit

async def basic_experiment():
    # Initialize toolkit with specific model
    toolkit = ChoiceToolkit(model_name="gpt-4o")
    
    # Run single experiment
    results = await toolkit.run_single_experiment("1.1_2.1", dataset, "mixed_450_qa")
    
    # Print results
    print(f"Experiment completed with {len(results)} samples")

# Run the experiment
asyncio.run(basic_experiment())
```

### ğŸ”¬ **Advanced Multi-Experiment Setup**

```python
async def comprehensive_evaluation():
    # Initialize toolkit
    toolkit = ChoiceToolkit(model_name="gpt-4o")
    
    # Define experiment suite
    experiments = ["1.1_2.1", "2.3", "2.6", "2.8"]
    
    # Run multiple experiments
    results = await toolkit.run_experiments(experiments)
    
    # Analyze results
    for exp_id, result in results.items():
        print(f"Experiment {exp_id}: {result['overall_accuracy']:.3f} accuracy")

# Execute comprehensive evaluation
asyncio.run(comprehensive_evaluation())
```

### ğŸ¤– **Multi-Model Comparison**

```python
async def model_comparison():
    models = ["gpt-4o", "claude-3-7-sonnet-20250219", "llama-3.1-8b-instruct"]
    
    for model in models:
        print(f"\nğŸ§ª Testing {model}...")
        toolkit = ChoiceToolkit(model_name=model)
        results = await toolkit.run_experiments(["1.1_2.1"])
        print(f"âœ… {model} completed")

# Run model comparison
asyncio.run(model_comparison())
```

### ğŸ“± **Command Line Interface**

```bash
# ğŸ¯ Run specific experiments
python main.py --experiment 1.1_2.1,2.3,2.6 --model gpt-4o

# ğŸš€ Run all experiments
python main.py --all --model claude-3-7-sonnet-20250219

# ğŸ“‹ List available experiments
python main.py --list

# âš¡ Interactive quick start
python quick_start.py

# ğŸ§ª Run test suite
python test_imports.py
```

### ğŸ“Š **Custom Dataset Usage**

```python
# Load custom dataset
from core.data_loader import ChoiceDataLoader

data_loader = ChoiceDataLoader(config)
custom_dataset, types, name = data_loader.load_and_prepare_dataset("your_dataset.json")

# Run experiments on custom data
toolkit = ChoiceToolkit()
results = await toolkit.run_single_experiment("2.3", custom_dataset, name)
```

## ğŸ¤– Supported Models

The toolkit supports a wide range of state-of-the-art language models:

<div align="center">

| ğŸ¢ Provider | ğŸ¤– Model | ğŸ¯ Recommended Use | âš¡ Speed |
|-------------|----------|-------------------|----------|
| **OpenAI** | GPT-4o | Primary evaluation | Fast |
| **OpenAI** | GPT-3.5-turbo | Cost-effective testing | Very Fast |
| **Anthropic** | Claude-3-7-sonnet-20250219 | Advanced reasoning | Fast |
| **Meta** | Llama-3.1-8b-instruct | Open-source alternative | Medium |
| **Google** | Gemini-2.0-flash | Latest capabilities | Fast |

</div>

> ğŸ’¡ **Tip**: Start with GPT-4o for best results, then use other models for comparison studies.

## ğŸ“š Research Applications

This toolkit is designed for research purposes in the following areas:

### ğŸ”¬ **Application Domains**
- **Cognitive Science**: Understanding LLM reasoning patterns
- **AI Safety**: Evaluating instruction following robustness
- **Prompt Engineering**: Optimizing prompt strategies
- **Model Comparison**: Benchmarking different LLMs
- **Bias Detection**: Identifying cognitive biases in AI systems

### ğŸ“– **Citation**

If you use this toolkit in your research, please cite this repository:

```bibtex
@software{choice_toolkit_2025,
  title={Choice Experiment Toolkit for LLM Evaluation},
  author={Anonymous},
  year={2025},
  url={https://github.com/ZpLing/Instruction-Boundary}
}
```

> **Note**: This is an anonymous submission. Full citation information will be provided upon publication.

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### ğŸ› **Bug Reports**
- Use GitHub Issues to report bugs
- Include reproduction steps and environment details

### ğŸš€ **Feature Requests**
- Suggest new experimental settings
- Propose additional evaluation metrics
- Request new model support

### ğŸ’» **Code Contributions**
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### ğŸ“– **Documentation**
- Improve README sections
- Add code examples
- Translate documentation

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**ğŸ¯ Choice Experiment Toolkit** - *Evaluating LLM Instruction Following Capabilities*

[â­ Star this repo](https://github.com/ZpLing/Instruction-Boundary) â€¢ [ğŸ› Report Bug](https://github.com/ZpLing/Instruction-Boundary/issues) â€¢ [ğŸ’¡ Request Feature](https://github.com/ZpLing/Instruction-Boundary/issues)

Made with â¤ï¸ for the AI research community

</div>