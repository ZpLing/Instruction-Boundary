#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Choice Toolkit - è¯„ä¼°æ¨¡å—
ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®—å’Œç»“æžœåˆ†æž
"""

from typing import List, Dict, Any
import json
import os

class ChoiceEvaluator:
    """Choiceè¯„ä¼°å™¨"""
    
    def __init__(self):
        pass
    
    def calculate_choice_metrics_with_tfu_style(self, results: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—ChoiceæŒ‡æ ‡ï¼ˆå­¦ä¹ TFUçš„Followå’ŒJumpæŒ‡æ ‡ï¼‰"""
        
        total_count = len(results)
        
        # ç»Ÿè®¡æå–æ–¹æ³•
        extraction_method_stats = {"keyword": 0, "llm_judge": 0}
        
        # æŒ‰é¢˜ç›®ç±»åž‹åˆ†ç»„
        single_choice_results = []
        multiple_choice_results = []
        no_answer_results = []
        
        # ç»Ÿè®¡LLMè¾“å‡ºåˆ†å¸ƒ
        llm_output_label_stats = {
            "0": 0, "1": 0, "2": 0, "3": 0,
            "0,1": 0, "0,2": 0, "0,3": 0, "1,2": 0, "1,3": 0, "2,3": 0,
            "NO_ANSWER": 0, "UNCLEAR": 0
        }
        
        for result in results:
            element = result['element']
            judge_eval = result['judge_evaluation']
            extraction_method = result['extraction_method']
            extracted_label = result['extracted_label']
            
            # ç»Ÿè®¡æå–æ–¹æ³•
            extraction_method_stats[extraction_method] = extraction_method_stats.get(extraction_method, 0) + 1
            
            # ç»Ÿè®¡LLMè¾“å‡ºåˆ†å¸ƒ
            if extracted_label in llm_output_label_stats:
                llm_output_label_stats[extracted_label] += 1
            
            # æŒ‰é¢˜ç›®ç±»åž‹åˆ†ç»„
            q_type = element.get('question_type', 'unknown')
            if q_type == 'single_choice':
                single_choice_results.append(result)
            elif q_type == 'multiple_choice':
                multiple_choice_results.append(result)
            elif q_type == 'no_correct_answer':
                no_answer_results.append(result)
        
        # 1. FollowçŽ‡ï¼šå•é€‰é¢˜å†™å¯¹äº†
        def calculate_follow_rate(single_choice_results):
            """è®¡ç®—FollowçŽ‡ï¼šå•é€‰é¢˜å†™å¯¹äº†"""
            follow_correct = 0
            follow_total = len(single_choice_results)
            
            for result in single_choice_results:
                judge_eval = result['judge_evaluation']
                if judge_eval['is_correct']:
                    follow_correct += 1
            
            follow_rate = follow_correct / follow_total if follow_total > 0 else 0
            return follow_rate, follow_correct, follow_total
        
        follow_rate, follow_correct, follow_total = calculate_follow_rate(single_choice_results)
        
        # 2. JumpçŽ‡1ï¼šå››é€‰0ï¼ˆæ— ç­”æ¡ˆï¼‰åˆ¤æ–­å¯¹äº†
        def calculate_jump_rate_no_answer(no_answer_results):
            """è®¡ç®—JumpçŽ‡1ï¼šå››é€‰0ï¼ˆæ— ç­”æ¡ˆï¼‰åˆ¤æ–­å¯¹äº†"""
            jump_correct = 0
            jump_total = len(no_answer_results)
            
            for result in no_answer_results:
                judge_eval = result['judge_evaluation']
                if judge_eval['is_correct'] and judge_eval['has_no_answer']:
                    jump_correct += 1
            
            jump_rate = jump_correct / jump_total if jump_total > 0 else 0
            return jump_rate, jump_correct, jump_total
        
        jump_rate_no_answer, jump_correct_no_answer, jump_total_no_answer = calculate_jump_rate_no_answer(no_answer_results)
        
        # 3. JumpçŽ‡2ï¼šå¤šé€‰é¢˜åˆ¤æ–­å¯¹äº†
        def calculate_jump_rate_with_answer(multiple_choice_results):
            """è®¡ç®—JumpçŽ‡2ï¼šå¤šé€‰é¢˜åˆ¤æ–­å¯¹äº†"""
            jump_correct = 0
            jump_total = len(multiple_choice_results)
            
            for result in multiple_choice_results:
                judge_eval = result['judge_evaluation']
                if judge_eval['is_correct']:
                    jump_correct += 1
            
            jump_rate = jump_correct / jump_total if jump_total > 0 else 0
            return jump_rate, jump_correct, jump_total
        
        jump_rate_with_answer, jump_correct_with_answer, jump_total_with_answer = calculate_jump_rate_with_answer(multiple_choice_results)
        
        # 4. æ€»ä½“å‡†ç¡®çŽ‡
        total_correct = sum(1 for result in results if result['judge_evaluation']['is_correct'])
        overall_accuracy = total_correct / total_count if total_count > 0 else 0
        
        # 5. è¾“å‡ºåˆ†å¸ƒç»Ÿè®¡
        output_distribution = {}
        ground_truth_counts = {
            "single_choice": len(single_choice_results),
            "multiple_choice": len(multiple_choice_results), 
            "no_correct_answer": len(no_answer_results)
        }
        
        # ç»Ÿè®¡è¾“å‡ºç±»åž‹åˆ†å¸ƒ
        output_type_stats = {
            "single_option": 0,      # è¾“å‡ºå•ä¸ªé€‰é¡¹
            "multiple_options": 0,   # è¾“å‡ºå¤šä¸ªé€‰é¡¹
            "no_answer": 0           # è¾“å‡ºNo Answerï¼ˆåŒ…æ‹¬UNCLEARã€UNCERTAINç­‰ï¼‰
        }
        
        for result in results:
            label = result['extracted_label']
            if label == "NO_ANSWER" or label == "UNCLEAR" or label == "UNCERTAIN" or label is None:
                output_type_stats["no_answer"] += 1
            elif "," in label:  # åŒ…å«é€—å·ï¼Œè¯´æ˜Žæ˜¯å¤šé€‰
                output_type_stats["multiple_options"] += 1
            else:  # å•ä¸ªé€‰é¡¹
                output_type_stats["single_option"] += 1
        
        # æŒ‰é¢˜ç›®ç±»åž‹ç»Ÿè®¡è¯¦ç»†åˆ†å¸ƒ
        for q_type in ["single_choice", "multiple_choice", "no_correct_answer"]:
            if q_type == "single_choice":
                type_results = single_choice_results
            elif q_type == "multiple_choice":
                type_results = multiple_choice_results
            else:
                type_results = no_answer_results
            
            if type_results:
                type_outputs = {}
                for result in type_results:
                    label = result['extracted_label']
                    type_outputs[label] = type_outputs.get(label, 0) + 1
                
                total_type_samples = len(type_results)
                output_distribution[q_type] = {}
                for output_label, count in type_outputs.items():
                    ratio = count / total_type_samples if total_type_samples > 0 else 0
                    output_distribution[q_type][output_label] = {
                        "count": count,
                        "ratio": ratio,
                        "percentage": ratio * 100
                    }
        
        return {
            "method": "tfu_style_metrics",
            "overall_accuracy": overall_accuracy,
            "total_count": total_count,
            "follow_rate": follow_rate,
            "follow_correct": follow_correct,
            "follow_total": follow_total,
            "jump_rate_no_answer": jump_rate_no_answer,
            "jump_correct_no_answer": jump_correct_no_answer,
            "jump_total_no_answer": jump_total_no_answer,
            "jump_rate_with_answer": jump_rate_with_answer,
            "jump_correct_with_answer": jump_correct_with_answer,
            "jump_total_with_answer": jump_total_with_answer,
            "extraction_method_stats": extraction_method_stats,
            "llm_output_label_stats": llm_output_label_stats,
            "output_type_stats": output_type_stats,  # æ–°å¢žï¼šè¾“å‡ºç±»åž‹ç»Ÿè®¡
            "output_distribution": output_distribution,
            "ground_truth_counts": ground_truth_counts
        }
    
    def save_experiment_results(self, results: List[Dict], metrics: Dict[str, Any], 
                              model_name: str, dataset_name: str, experiment_type: str,
                              output_folder: str) -> Dict[str, str]:
        """ä¿å­˜å®žéªŒç»“æžœ"""
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_folder, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        clean_dataset_name = os.path.basename(dataset_name).replace('.json', '')
        
        # ä¿å­˜è¯¦ç»†ç»“æžœ
        evaluation_filename = os.path.join(
            output_folder, 
            f"{model_name}_{clean_dataset_name}_choice_{experiment_type}_evaluation.json"
        )
        
        with open(evaluation_filename, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜å‡†ç¡®çŽ‡æ•°æ®
        accuracy_filename = os.path.join(
            output_folder,
            f"{model_name}_{clean_dataset_name}_choice_{experiment_type}_accuracy.json"
        )
        
        with open(accuracy_filename, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… {experiment_type}ç»“æžœå·²ä¿å­˜: {evaluation_filename}")
        print(f"âœ… {experiment_type}æŒ‡æ ‡å·²ä¿å­˜: {accuracy_filename}")
        
        return {
            "evaluation_file": evaluation_filename,
            "accuracy_file": accuracy_filename
        }
    
    def print_experiment_summary(self, metrics: Dict[str, Any], experiment_type: str):
        """æ‰“å°å®žéªŒæ€»ç»“"""
        print(f"\nðŸ“Š {experiment_type}å®žéªŒTFUé£Žæ ¼æŒ‡æ ‡:")
        print(f"   æ€»ä½“å‡†ç¡®çŽ‡: {metrics['overall_accuracy']:.3f} ({metrics['total_count']}ä¸ªæ ·æœ¬)")
        print(f"   FollowçŽ‡: {metrics['follow_rate']:.3f} ({metrics['follow_correct']}/{metrics['follow_total']}) [å•é€‰é¢˜å†™å¯¹äº†]")
        print(f"   JumpçŽ‡1: {metrics['jump_rate_no_answer']:.3f} ({metrics['jump_correct_no_answer']}/{metrics['jump_total_no_answer']}) [æ— ç­”æ¡ˆé¢˜åˆ¤æ–­å¯¹äº†]")
        print(f"   JumpçŽ‡2: {metrics['jump_rate_with_answer']:.3f} ({metrics['jump_correct_with_answer']}/{metrics['jump_total_with_answer']}) [å¤šé€‰é¢˜åˆ¤æ–­å¯¹äº†]")
        print(f"   æ ‡ç­¾æå–æ–¹æ³•: {metrics['extraction_method_stats']}")
        
        # è¾“å‡ºç±»åž‹ç»Ÿè®¡
        print(f"\nðŸ“ˆ è¾“å‡ºç±»åž‹åˆ†å¸ƒ (æ€»è®¡{metrics['total_count']}ä¸ªæ ·æœ¬):")
        print(f"   æ•°æ®é›†ç»„æˆ: {metrics['ground_truth_counts']['single_choice']}ä¸ªå•é€‰é¢˜ + {metrics['ground_truth_counts']['multiple_choice']}ä¸ªå¤šé€‰é¢˜ + {metrics['ground_truth_counts']['no_correct_answer']}ä¸ªæ— ç­”æ¡ˆé¢˜")
        output_type_stats = metrics['output_type_stats']
        total_samples = metrics['total_count']
        for output_type, count in output_type_stats.items():
            if count > 0:
                percentage = count / total_samples * 100
                type_name = {
                    "single_option": "å•ä¸ªé€‰é¡¹",
                    "multiple_options": "å¤šä¸ªé€‰é¡¹", 
                    "no_answer": "æ— ç­”æ¡ˆ"
                }.get(output_type, output_type)
                print(f"     {type_name:>10}: {count:>3}æ¬¡ ({percentage:>5.1f}%)")
        
        # è¯¦ç»†è¾“å‡ºåˆ†å¸ƒç»Ÿè®¡
        print(f"\nðŸ“ˆ è¯¦ç»†LLMè¾“å‡ºåˆ†å¸ƒ:")
        for label, count in metrics['llm_output_label_stats'].items():
            if count > 0:
                percentage = count / metrics['total_count'] * 100
                print(f"     {label:>10}: {count:>3}æ¬¡ ({percentage:>5.1f}%)")
        
        # æŒ‰é¢˜ç›®ç±»åž‹çš„è¾“å‡ºåˆ†å¸ƒ
        print(f"\nðŸ“Š æŒ‰é¢˜ç›®ç±»åž‹çš„è¾“å‡ºåˆ†å¸ƒ:")
        for q_type, distribution in metrics['output_distribution'].items():
            print(f"\n   {q_type}:")
            for output_label, stats in distribution.items():
                print(f"     {output_label:>10}: {stats['count']:>3}æ¬¡ ({stats['percentage']:>5.1f}%)")

