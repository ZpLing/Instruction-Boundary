#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Choice Toolkit - APIå®¢æˆ·ç«¯æ¨¡å—
ç»Ÿä¸€çš„APIè°ƒç”¨å’ŒLLM JudgeåŠŸèƒ½
"""

import asyncio
import openai
from typing import List, Dict, Any, Tuple
from .utils import extract_choice_label_by_keywords, improved_choice_judge_logic

class ChoiceAPIClient:
    """Choice APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.client = openai.AsyncOpenAI(
            api_key=config["api_key"],
            base_url=config["base_url"]
        )
        self.semaphore = asyncio.Semaphore(config.get("semaphore_limit", 50))
    
    async def call_openai(self, msg: List[Dict]) -> str:
        """è°ƒç”¨OpenAI API"""
        async with self.semaphore:
            try:
                response = await self.client.chat.completions.create(
                    model=self.config["test_model"],
                    messages=msg,
                    max_tokens=self.config.get("max_tokens", 80),
                    temperature=self.config.get("temperature", 0.0)
                )
                content = response.choices[0].message.content
                return content.strip() if content else "ERROR"
            except Exception as e:
                print(f"APIè°ƒç”¨é”™è¯¯: {e}")
                return "ERROR"
    
    async def llm_judge_extract_choice_label(self, answer_text: str) -> str:
        """ä½¿ç”¨LLM judgeæ¥æå–Choiceæ ‡ç­¾"""
        judge_prompt = f"""Analyze the following LLM response and determine which option(s) it selected:

Response content: "{answer_text}"

Please determine what this response expresses:
- Selected option 0: respond "0"
- Selected option 1: respond "1" 
- Selected option 2: respond "2"
- Selected option 3: respond "3"
- Selected multiple options (e.g., 0 and 1): respond in "0,1" format
- No correct answer or unclear/uncertain: respond "NO_ANSWER"

Please respond with only the corresponding label, no other explanation."""

        try:
            response = await self.client.chat.completions.create(
                model=self.config["judge_model"],  # ä½¿ç”¨gpt-4oä½œä¸ºJudge
                messages=[{"role": "user", "content": judge_prompt}],
                max_tokens=20,
                temperature=0.0
            )
            
            content = response.choices[0].message.content
            judge_result = content.strip() if content else "UNCLEAR"
            return judge_result
            
        except Exception as e:
            print(f"âš ï¸  LLM Judgeè°ƒç”¨é”™è¯¯: {e}")
            return "UNCLEAR"  # é»˜è®¤è¿”å›UNCLEAR
    
    async def hybrid_extract_choice_label(self, answer_text: str) -> Tuple[str, str]:
        """æ··åˆç­–ç•¥ï¼šå…ˆå°è¯•å…³é”®è¯åŒ¹é…ï¼Œæ— æ³•è¯†åˆ«æ—¶ä½¿ç”¨LLM judge"""
        # å…ˆå°è¯•å…³é”®è¯åŒ¹é…
        keyword_result = extract_choice_label_by_keywords(answer_text)
        if keyword_result is not None:
            return keyword_result, "keyword"
        
        # å…³é”®è¯æ— æ³•è¯†åˆ«ï¼Œä½¿ç”¨LLM judge
        llm_result = await self.llm_judge_extract_choice_label(answer_text)
        return llm_result, "llm_judge"
    
    async def process_single_element(self, element: Dict[str, Any], prompt_builder, 
                                   index: int, experiment_type: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªå…ƒç´ """
        # æ„å»ºprompt
        prompt_content = prompt_builder(element)
        
        msg = [{
            "role": "user",
            "content": prompt_content
        }]
        
        # è·å–æ¨¡å‹å›ç­”
        answer = await self.call_openai(msg)
        
        # ä½¿ç”¨æ··åˆç­–ç•¥æå–æ ‡ç­¾
        extracted_label, extraction_method = await self.hybrid_extract_choice_label(answer)
        
        # ä½¿ç”¨æ”¹è¿›çš„judgingé€»è¾‘è¯„ä¼°
        judge_result = improved_choice_judge_logic(answer, element.get('correct_answers', []))
        
        return {
            "index": index,
            "element": element,
            "response": answer,
            "prompt_type": experiment_type,
            "prompt": prompt_content,
            "extracted_label": extracted_label,
            "extraction_method": extraction_method,
            "judge_evaluation": judge_result
        }
    
    async def process_dataset(self, dataset: List[Dict], prompt_builder, 
                            experiment_type: str) -> List[Dict]:
        """å¤„ç†æ•´ä¸ªæ•°æ®é›†"""
        print(f"\nğŸš€ è¿è¡ŒChoice {experiment_type}å®éªŒ - æ¨¡å‹: {self.config['test_model']}")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"   Judgeæ¨¡å‹: {self.config['judge_model']}")
        
        # åˆ›å»ºä»»åŠ¡
        tasks = []
        for i, element in enumerate(dataset):
            task = self.process_single_element(element, prompt_builder, i, experiment_type)
            tasks.append(task)
        
        # æ‰§è¡Œä»»åŠ¡
        results = []
        for task in asyncio.as_completed(tasks):
            result = await task
            results.append(result)
        
        # æŒ‰ç´¢å¼•æ’åº
        results.sort(key=lambda x: x.get('index', 0))
        
        return results
