#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Choice Toolkit - æ•°æ®åŠ è½½æ¨¡å—
ç»Ÿä¸€çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†åŠŸèƒ½
"""

import json
import os
from typing import List, Dict, Any, Tuple
from .utils import validate_choice_element

class ChoiceDataLoader:
    """Choiceæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.dataset_config = config.get("dataset", {})
        
    def load_dataset_config(self, config_file: str = "choice_config.json") -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é›†é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {
                "datasets": {
                    "mixed_450_qa": {
                        "question_types": ["single_choice", "multiple_choice", "no_correct_answer"],
                        "description": "450ä¸ªæ ·æœ¬çš„æ··åˆé€‰æ‹©é¢˜æ•°æ®é›†",
                        "file_patterns": ["mixed_450_qa_dataset.json"],
                    }
                },
                "default_config": {
                    "question_types": ["single_choice", "multiple_choice", "no_correct_answer"],
                    "description": "é»˜è®¤Choiceé…ç½®"
                }
            }
    
    def get_all_dataset_files(self) -> List[str]:
        """è·å–æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶"""
        dataset_files = []
        
        # æ£€æŸ¥å½“å‰ç›®å½•å’Œtoolkitç›®å½•
        possible_paths = [
            'mixed_450_qa_dataset.json',
            './mixed_450_qa_dataset.json',
            'choice_toolkit/mixed_450_qa_dataset.json'
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                dataset_files = [path]
                print(f"âœ… æ‰¾åˆ°450ä¸ªæ ·æœ¬çš„é€‰æ‹©é¢˜æ•°æ®é›†: {path}")
                break
        
        if not dataset_files and os.path.exists('choice_tfu_format_dataset.json'):
            dataset_files = ['choice_tfu_format_dataset.json']
            print(f"âœ… æ‰¾åˆ°TFUæ ¼å¼çš„450ä¸ªæ ·æœ¬æ•°æ®é›†: choice_tfu_format_dataset.json")
        elif not dataset_files:
            print("âŒ æœªæ‰¾åˆ°450ä¸ªæ ·æœ¬çš„æ•°æ®é›†æ–‡ä»¶")
        
        return dataset_files
    
    def validate_and_normalize_choice_types(self, dataset: List[Dict], expected_types: List[str]) -> List[str]:
        """éªŒè¯æ•°æ®é›†ä¸­çš„é¢˜ç›®ç±»å‹å¹¶è¿”å›æ ‡å‡†åŒ–åˆ—è¡¨"""
        existing_types = set()
        for element in dataset:
            if "question_type" in element:
                existing_types.add(element["question_type"])
        
        print(f"æ•°æ®é›†ä¸­å‘ç°çš„é¢˜ç›®ç±»å‹: {list(existing_types)}")
        
        if existing_types and not existing_types.issubset(set(expected_types)):
            print(f"è­¦å‘Š: æ•°æ®é›†ä¸­çš„é¢˜ç›®ç±»å‹ä¸é¢„æœŸä¸åŒ¹é…ï¼Œä½¿ç”¨æ•°æ®é›†ä¸­çš„ç±»å‹")
            return list(existing_types)
        
        return expected_types
    
    def load_and_prepare_dataset(self, dataset_file: str, config: Dict[str, Any]) -> Tuple[List[Dict], List[str], str]:
        """åŠ è½½å’Œå‡†å¤‡å•ä¸ªChoiceæ•°æ®é›†"""
        print(f"\nğŸ“ åŠ è½½æ•°æ®é›†: {dataset_file}")
        
        # è·å–æ•°æ®é›†é…ç½®
        dataset_name = dataset_file.replace('.json', '')
        dataset_config = config.get("datasets", {}).get("mixed_450_qa", config.get("default_config", {}))
        print(f"æ•°æ®é›†é…ç½®: {dataset_config.get('description', 'é»˜è®¤é…ç½®')}")
        
        # åŠ è½½æ•°æ®
        with open(dataset_file, "r") as file:
            raw_data = json.load(file)
        
        # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
        dataset = []
        for element in raw_data:
            if validate_choice_element(element):
                dataset.append(element)
        
        # éªŒè¯å¹¶æ ‡å‡†åŒ–é¢˜ç›®ç±»å‹
        question_types = dataset_config.get("question_types", ["single_choice", "multiple_choice", "no_correct_answer"])
        question_types = self.validate_and_normalize_choice_types(dataset, question_types)
        
        print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(dataset)}")
        print(f"æœ€ç»ˆä½¿ç”¨çš„é¢˜ç›®ç±»å‹: {question_types}")
        
        return dataset, question_types, dataset_name
    
    def get_dataset_statistics(self, dataset: List[Dict]) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_samples": len(dataset),
            "question_types": {},
            "dataset_sources": {},
            "num_options_distribution": {}
        }
        
        for element in dataset:
            # ç»Ÿè®¡é¢˜ç›®ç±»å‹
            q_type = element.get('question_type', 'unknown')
            stats["question_types"][q_type] = stats["question_types"].get(q_type, 0) + 1
            
            # ç»Ÿè®¡æ•°æ®æ¥æº
            source = element.get('dataset_source', 'unknown')
            stats["dataset_sources"][source] = stats["dataset_sources"].get(source, 0) + 1
            
            # ç»Ÿè®¡é€‰é¡¹æ•°é‡
            num_options = len(element.get('options', []))
            stats["num_options_distribution"][num_options] = stats["num_options_distribution"].get(num_options, 0) + 1
        
        return stats
    
    def filter_dataset_by_type(self, dataset: List[Dict], question_types: List[str]) -> List[Dict]:
        """æŒ‰é¢˜ç›®ç±»å‹è¿‡æ»¤æ•°æ®é›†"""
        filtered_dataset = []
        for element in dataset:
            if element.get('question_type') in question_types:
                filtered_dataset.append(element)
        return filtered_dataset
    
    def split_dataset_by_type(self, dataset: List[Dict]) -> Dict[str, List[Dict]]:
        """æŒ‰é¢˜ç›®ç±»å‹åˆ†å‰²æ•°æ®é›†"""
        split_data = {
            "single_choice": [],
            "multiple_choice": [],
            "no_correct_answer": []
        }
        
        for element in dataset:
            q_type = element.get('question_type', 'unknown')
            if q_type in split_data:
                split_data[q_type].append(element)
        
        return split_data
