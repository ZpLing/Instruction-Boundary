#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Choice Toolkit - æ•°æ®åŠ è½½æ¨¡å—
ç»Ÿä¸€çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†åŠŸèƒ½
"""

import json
import os
from typing import List, Dict, Any, Tuple
from .utils import validate_choice_element

class ChoiceDataLoader:
    """Choiceæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.dataset_config = config.get("dataset", {})
        
    def load_dataset_config(self, config_file: str = "choice_config.json") -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é›†é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {
                "datasets": {
                    "mixed_450_qa": {
                        "question_types": ["single_choice", "multiple_choice", "no_correct_answer"],
                        "description": "450ä¸ªæ ·æœ¬çš„æ··åˆé€‰æ‹©é¢˜æ•°æ®é›†",
                        "file_patterns": ["mixed_450_qa_dataset.json"],
                    }
                },
                "default_config": {
                    "question_types": ["single_choice", "multiple_choice", "no_correct_answer"],
                    "description": "é»˜è®¤Choiceé…ç½®"
                }
            }
    
    def get_all_dataset_files(self) -> List[str]:
        """è·å–æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶"""
        dataset_files = []
        
        # æ£€æŸ¥Choiceæ ¼å¼æ•°æ®é›†
        choice_paths = [
            'mixed_450_qa_dataset.json',
            './mixed_450_qa_dataset.json',
            'choice_toolkit/mixed_450_qa_dataset.json'
        ]
        
        for path in choice_paths:
            if os.path.exists(path):
                dataset_files.append(path)
                print(f"âœ… æ‰¾åˆ°Choiceæ ¼å¼æ•°æ®é›†: {path}")
                break
        
        # æ£€æŸ¥TFUæ ¼å¼æ•°æ®é›†
        tfu_paths = [
            'choice_tfu_format_dataset.json',
            './choice_tfu_format_dataset.json',
            '../Choices/choice_tfu_format_dataset.json'
        ]
        
        for path in tfu_paths:
            if os.path.exists(path):
                dataset_files.append(path)
                print(f"âœ… æ‰¾åˆ°TFUæ ¼å¼æ•°æ®é›†: {path}")
                break
        
        if not dataset_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†æ–‡ä»¶")
        
        return dataset_files
    
    def validate_and_normalize_choice_types(self, dataset: List[Dict], expected_types: List[str]) -> List[str]:
        """éªŒè¯æ•°æ®é›†ä¸­çš„é¢˜ç›®ç±»å‹å¹¶è¿”å›æ ‡å‡†åŒ–åˆ—è¡¨"""
        existing_types = set()
        for element in dataset:
            if "question_type" in element:
                existing_types.add(element["question_type"])
        
        print(f"æ•°æ®é›†ä¸­å‘ç°çš„é¢˜ç›®ç±»å‹: {list(existing_types)}")
        
        if existing_types and not existing_types.issubset(set(expected_types)):
            print(f"è­¦å‘Š: æ•°æ®é›†ä¸­çš„é¢˜ç›®ç±»å‹ä¸é¢„æœŸä¸åŒ¹é…ï¼Œä½¿ç”¨æ•°æ®é›†ä¸­çš„ç±»å‹")
            return list(existing_types)
        
        return expected_types
    
    def detect_dataset_format(self, dataset_file: str) -> str:
        """æ£€æµ‹æ•°æ®é›†æ ¼å¼"""
        with open(dataset_file, "r") as file:
            raw_data = json.load(file)
        
        if not raw_data:
            return "unknown"
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å­—æ®µ
        first_element = raw_data[0]
        
        # TFUæ ¼å¼ç‰¹å¾ï¼šæœ‰Conclusionå’ŒFactså­—æ®µ
        if "Conclusion" in first_element and "Facts" in first_element:
            return "tfu"
        # Choiceæ ¼å¼ç‰¹å¾ï¼šæœ‰questionå’Œoptionså­—æ®µ
        elif "question" in first_element and "options" in first_element:
            return "choice"
        else:
            return "unknown"
    
    def convert_tfu_to_choice_format(self, tfu_element: Dict[str, Any]) -> Dict[str, Any]:
        """å°†TFUæ ¼å¼è½¬æ¢ä¸ºChoiceæ ¼å¼"""
        # TFUæ ¼å¼å·²ç»åŒ…å«optionså­—æ®µï¼Œç›´æ¥ä½¿ç”¨
        options = tfu_element.get("options", [])
        
        # æ„å»ºChoiceæ ¼å¼
        choice_element = {
            "question": tfu_element.get("Conclusion", ""),
            "options": options,
            "correct_answers": tfu_element.get("correct_answers", []),
            "question_type": tfu_element.get("question_type", "single_choice"),
            "dataset_source": tfu_element.get("dataset_source", "unknown"),
            "original_id": tfu_element.get("original_id", ""),
            "num_options": len(options),
            "num_correct": len(tfu_element.get("correct_answers", [])),
            "proof_label": tfu_element.get("proof_label", ""),
            # ä¿ç•™TFUåŸå§‹å­—æ®µç”¨äºpromptæ„å»º
            "facts": tfu_element.get("Facts", ""),
            "conclusion": tfu_element.get("Conclusion", "")
        }
        
        return choice_element
    
    def load_and_prepare_dataset(self, dataset_file: str, config: Dict[str, Any]) -> Tuple[List[Dict], List[str], str]:
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†ï¼ˆæ”¯æŒChoiceå’ŒTFUæ ¼å¼ï¼‰"""
        print(f"\nğŸ“ åŠ è½½æ•°æ®é›†: {dataset_file}")
        
        # æ£€æµ‹æ•°æ®é›†æ ¼å¼
        dataset_format = self.detect_dataset_format(dataset_file)
        print(f"æ£€æµ‹åˆ°æ•°æ®é›†æ ¼å¼: {dataset_format}")
        
        # è·å–æ•°æ®é›†é…ç½®
        dataset_name = dataset_file.replace('.json', '')
        dataset_config = config.get("datasets", {}).get("mixed_450_qa", config.get("default_config", {}))
        print(f"æ•°æ®é›†é…ç½®: {dataset_config.get('description', 'é»˜è®¤é…ç½®')}")
        
        # åŠ è½½æ•°æ®
        with open(dataset_file, "r") as file:
            raw_data = json.load(file)
        
        # æ ¹æ®æ ¼å¼å¤„ç†æ•°æ®
        dataset = []
        for element in raw_data:
            if dataset_format == "tfu":
                # è½¬æ¢TFUæ ¼å¼ä¸ºChoiceæ ¼å¼
                choice_element = self.convert_tfu_to_choice_format(element)
                if validate_choice_element(choice_element):
                    dataset.append(choice_element)
            elif dataset_format == "choice":
                # ç›´æ¥ä½¿ç”¨Choiceæ ¼å¼
                if validate_choice_element(element):
                    dataset.append(element)
            else:
                print(f"âš ï¸ æœªçŸ¥æ•°æ®é›†æ ¼å¼: {dataset_format}")
                continue
        
        # éªŒè¯å¹¶æ ‡å‡†åŒ–é¢˜ç›®ç±»å‹
        question_types = dataset_config.get("question_types", ["single_choice", "multiple_choice", "no_correct_answer"])
        question_types = self.validate_and_normalize_choice_types(dataset, question_types)
        
        print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(dataset)}")
        print(f"æœ€ç»ˆä½¿ç”¨çš„é¢˜ç›®ç±»å‹: {question_types}")
        print(f"æ•°æ®é›†æ ¼å¼: {dataset_format}")
        
        return dataset, question_types, dataset_name
    
    def get_dataset_statistics(self, dataset: List[Dict]) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_samples": len(dataset),
            "question_types": {},
            "dataset_sources": {},
            "num_options_distribution": {}
        }
        
        for element in dataset:
            # ç»Ÿè®¡é¢˜ç›®ç±»å‹
            q_type = element.get('question_type', 'unknown')
            stats["question_types"][q_type] = stats["question_types"].get(q_type, 0) + 1
            
            # ç»Ÿè®¡æ•°æ®æ¥æº
            source = element.get('dataset_source', 'unknown')
            stats["dataset_sources"][source] = stats["dataset_sources"].get(source, 0) + 1
            
            # ç»Ÿè®¡é€‰é¡¹æ•°é‡
            num_options = len(element.get('options', []))
            stats["num_options_distribution"][num_options] = stats["num_options_distribution"].get(num_options, 0) + 1
        
        return stats
    
    def filter_dataset_by_type(self, dataset: List[Dict], question_types: List[str]) -> List[Dict]:
        """æŒ‰é¢˜ç›®ç±»å‹è¿‡æ»¤æ•°æ®é›†"""
        filtered_dataset = []
        for element in dataset:
            if element.get('question_type') in question_types:
                filtered_dataset.append(element)
        return filtered_dataset
    
    def split_dataset_by_type(self, dataset: List[Dict]) -> Dict[str, List[Dict]]:
        """æŒ‰é¢˜ç›®ç±»å‹åˆ†å‰²æ•°æ®é›†"""
        split_data = {
            "single_choice": [],
            "multiple_choice": [],
            "no_correct_answer": []
        }
        
        for element in dataset:
            q_type = element.get('question_type', 'unknown')
            if q_type in split_data:
                split_data[q_type].append(element)
        
        return split_data
